<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Viet Cuong Truong</title>
    <link rel="shortcut icon" href="../images/a.jpg" type="image/x-icon">
    <link rel="icon" href="../images/a" type="image/x-icon">
    <link href='https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css' rel='stylesheet'>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../project contents/subpage3-style.css">

</head>

<body>

    <header class="header">
        <a href="../" class="logo">Portfolio.</a>

        <div class='bx bx-menu' id="menu-icon"></div>

        <nav class="navbar">
            <a href="../#home">Home</a>
            <a href="../#about">About</a>
            <a href="../#education">Education</a>
            <a href="../#portfolio" class="active">Projects</a>
            <a href="../#contact">Contact</a>
        </nav>
    </header>

    <section class="projects" id="projects">
        <div class="content-container">
        <h2 class="heading"> Clustering for<span> Effective Marketing Strategy</span></h2>
        <img class="heading-image" src="../images/subpage3/Cover image - Waffle plot .png" alt="#" />
        <!-- Add your project content here -->
        <br>
        <br>
            <h3>Introduction</h3>
                <p>Customer Personality Analysis is a comprehensive analysis of an organization's ideal clients. It aids businesses in gaining a better understanding of their customers and makes it easier for them to tailor their products to the specific needs, behaviours, and concerns of various customer types.</p>
                <p>Analysing the personalities of customers enables a business to modify its product based on its target customers from various customer segments. For instance, instead of spending money to market a new product to every customer in the company's database, the company can analyse which customer segment is most likely to purchase the product and focus marketing efforts on that segment alone.</p>
                <br>
                <p><strong>Objectives:</strong></p>
                <p>
                    In this project, we will be utilizing  <a href="https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis"target="_blank">this dataset</a> to perform clustering analysis. The aim is to identify distinct clusters of customers based on their personality traits and characteristics. This will allow us to develop tailored marketing strategies for each cluster, optimizing our efforts to effectively target and engage different customer segments. Our objectives include:
                </p>
                <ul>
                    <li>Grouping customers into clusters using various clustering models.</li>
                    <li>Perform interpretation and analysis of the groups (profiling) that have been created.</li>
                    <li>Provide marketing suggestions based on profiling results and analysis conducted.</li>
                </ul>
                <br>
                <p><strong>Clustering Models:</strong></p>
                <ul>
                    <li>Partition based (K-Means)</li>
                    <li>Density based (DBSCAN)</li>
                    <li>Hierarchical Clustering (Agglomerative)</li>
                </ul>
                <br>
                <p><strong>Dataset Description:</strong></p>
                <img class="plot-image" src="../images/subpage3/Data Description.png" />
            
           <h3>Data Cleaning</h3>
                <p>Perform the following data cleaning and feature engineering steps:</p>
                <ul>
                        <li>Eliminate null values: Clean the dataset to make it more reliable.</li>
                        <li>Derive "Age": Calculate the age of a customer from the "Year_Birth" field, which indicates the customer's birth year.</li>
                        <li>Generate "Spent": Create a new feature to represent the total amount spent by a customer across different categories over two years.</li>
                        <li>Create "Living_With": Use the "Marital_Status" field to identify the living situation of couples.</li>
                        <li>Create "Children": Establish a feature to represent the total number of children in a household, including both kids and teenagers.</li>
                        <li>Establish "Family_Size": Create a feature to indicate the total number of family members in a household.</li>
                        <li>Introduce "Is_Parent": Implement a feature to indicate whether a customer is a parent.</li>
                        <li>Simplify "Education": Divide this attribute into three categories to make it more manageable and understandable.</li>
                        <li>Discard unnecessary features: Remove redundant features that could complicate the analysis and prediction process.</li>
                        <li>Eliminate outliers: Remove outliers from the dataset to ensure more accurate statistical analyses and machine learning predictions.</li>                   
                </ul>
                <img class="plot-image-cleaning" src="../images/subpage3/data cleaning.png" />
            
            <h3>Data Preprocessing</h3>
                <p>This section will perform some pre-processing before using clustering models.</p>
                <p><strong> Dropping Variables</strong></p>
                <p>The first stage is to remove variables that are not needed for the clustering process. In this case 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response' will be removed.</p>
                <br>
                <p><strong>Scaling</strong></p>
                <p>Our subsequent stage involves scaling the dataset. Scaling is of paramount importance as it handles the dispersion in the dataset, applies a linear transformation to convert the data into a specific range, and consequently enhances the effectiveness of clustering algorithms, resulting in more reliable clusters. Here, we will employ a standard scaler, which normalizes the features by subtracting the mean and scaling to unit variance, ensuring that the data attributes have a uniform scale.</p>
                <br>
                <p><strong>Hopkins Test</strong></p>
                <p>The next step is to perform a statistical test using the Hopkins statistical test for the preprocessed dataset. This test measures the clustering tendency of data, or in other words, the extent to which meaningful clusters exist in the data set to be clustered.</p>

                <p>The hypothesis of the Hopkins statistical test is as follows:</p>
                <ul>
                <li><strong>H0 (Null Hypothesis):</strong> The dataset is not uniformly distributed (contains meaningful clusters).</li>
                <li><strong>H1 (Alternative Hypothesis):</strong> The dataset is uniformly distributed (lacks meaningful clusters).</li>
                </ul>
                
                <p>The criteria for the Hopkins statistical test is:</p>
                <p>If the value is between {0.7, ..., 0.99}, we accept H0, indicating the data has a high tendency to cluster.</p>
                <img class="plot-image" src="../images/subpage3/Hopkin test.png" />
                <br>
                <p><strong>Principal component analysis</strong></p>
                <p>Principal Component Analysis (PCA) is a technique used in unsupervised machine learning approaches, such as clustering. Its main purpose is to transform high-dimensional data into a smaller number of dimensions, while preserving as much significant information as possible. The use of PCA prior to deploying a clustering algorithm helps in reducing the dimensionality, minimizing data noise, and lessening computational cost.</p>
                <p>In this particular project, we'll be reducing the number of features to two dimensions. This reduction enables us to effectively visualize the clustering outcomes.</p>
                <img class="plot-evaluate" src="../images/subpage3/PCA.png" />     
                <br>
                <br>

            <h3>Clustering</h3>
                <p><strong style="font-size:24px;">K-Means</strong></p>
                <p>K-Means clustering is a straightforward yet potent algorithm utilized in unsupervised learning to tackle clustering issues. The methodology involves segregating a given dataset into a certain number of clusters, denoted by "k". These clusters are marked as points, and all observations or data points are associated with their closest cluster. Subsequent to this, calculations and adjustments are performed. The process is then reiterated with these fresh adjustments until the desired result is obtained.</p>
                <img class="plot-example-image" src="../images/subpage3/K-Mean.png" />  
                <p style="text-align:center; font-size:13px;"><em>ðŸ–¼ K-Means Clustering by <a href="https://pranshu453.medium.com/k-means-clustering-simplified-in-python-ab57b8d629db"target="_blank">Pranshu Sharma</a></em></p>
                <p>Prior to implementing K-Means, the first step is to determine the optimal number of clusters. This can be achieved using the elbow score. In addition, the Calinski-Harabasz index will be employed to help establish the ideal number of clusters.</p>
                <img class="plot-image" src="../images/subpage3/a.png" />
                <p>According to the results of the elbow method and the Calinski Harabasz score, the optimal number of clusters for the K-Means algorithm is four. The subsequent steps will apply the optimal number of clusters, visualise the clusters distribution plot, and evaluate their performance using silhouette plots.</p>
                <img class="plot-image" src="../images/subpage3/b.png" />
                <p>The provided image depicts an analysis of the data clustered into four distinct groups: cluster 1, cluster 2, cluster 3, and cluster 4. These clusters account for 25.95%, 22.51%, 22.88%, and 28.66%, respectively, of the data points, demonstrating a relatively even distribution across the clusters.</p>
                <p>In the silhouette plot, silhouette coefficient values for each cluster are displayed. All clusters are found to be higher than the average silhouette score, indicating that the clustering solution is suitable. In addition, the thickness of the silhouette plots reveals a similar range of variation across all clusters, indicating that the clustering solution is consistent. Notably, clusters 2 and 4 have a slightly thicker silhouette, indicating greater cohesion within these clusters.</p>
                <p>The waffle chart at the bottom provides a clear visual representation of the customer distribution across clusters by displaying the percentage of customers in each cluster.</p>
                <p>The quality of this clustering solution will be evaluated in the future using a variety of metrics, including the Davies-Bouldin index, silhouette score, and Calinski-Harabasz index.</p>
                <img class="plot-evaluate" src="../images/subpage3/Kmeans Evaluate.png" />
                <br>
                <br>

                <p><strong style="font-size:24px;">DBScan</strong></p>
                <p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clusters points according to the minimum number of points and the Euclidean distance. It also identifies as outliers points in low-density regions. The two parameters of DBSCAN are MinPoints and Epsilon.</p>
                <img class="plot-example-image" src="../images/subpage3/DBSCAN.png" />  
                <p style="text-align:center; font-size:13px;"><em>ðŸ–¼ DBScan Clustering by <a href="https://www.researchgate.net/figure/The-DBSCAN-algorithm-and-two-generated-clusters-There-are-three-types-of-points-as_fig2_342082665"target="_blank">David A. Bonneau</a></em></p>
                <p>Before applying the DBSCAN algorithm, we must define the previously mentioned DBSCAN parameters. Since PCA has already been performed in two dimensions, we will use the default value (4) for MinPoint. For Epsilon values, we will determine the distance between each data point and its nearest neighbour using Nearest Neighbours, and then sort them before plotting them. Using the plot, we can then determine the greatest value at the graph's curve.</p>
                <img class="plot-image" src="../images/subpage3/DBScan plot.png" />
                <p>The subsequent step is to implement DBSCAN and evaluate the results based on the results of the maximum curvature and the previous MinPoint values.</p>
                <img class="plot-image" src="../images/subpage3/DBscan 8 clusters.png" />
                <p>From the implementation of DBSCAN, eight clusters are formed. Clusters 1 and 2 have more data points than other clusters. However, there are some outliers detected because some points are too far from the other data points (DBSCAN labelled those points as outliers with a -1 value). The next step is to evaluate the clustering quality provided by DBSCAN.</p>
                <img class="plot-evaluate" src="../images/subpage3/DBSCAN Eva.png" />
                <br>
                <p>In summary, the DBSCAN clustering results show moderate performance according to Davies-Bouldin Index and Calinski Harabasz Index, but poor performance based on the Silhouette Score. This suggests that some data points might not have been correctly assigned to their clusters, leading to overlap between clusters.</p>
                <br>
                <br>

                <p><strong style="font-size:24px;">Hierarchical Clustering</strong></p>
                <p>Hierarchical clustering organises data into a clustering tree. Hierarchical clustering begins with each data point being treated as a separate cluster. Then, it repeatedly identifies the two clusters that are most similar and merges them until all of the clusters have been merged together. The objective of hierarchical clustering is to generate a hierarchical series of nested clusters. Dendrograms will be utilised to visualise the evolution of groupings and determine the optimal cluster size. Using the generated dendograms, we then determine the greatest vertical distance that does not intersect any other clusters. After that, draw a horizontal threshold line at both ends. The optimal number of clusters equals the number of vertical lines traversing the horizontal line. In the example given below, the optimal number of clusters would be four.</p>
                <img class="plot-example-image" src="../images/subpage3/Hiarachical Clustering Example.png" />   
                <p style="text-align:center; font-size:13px;"><em>ðŸ–¼ Dendogram and Hierarchical Clustering Result by <a href="https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8"target="_blank">Prasad Pai</a></em></p> 
                <p>The initial step in our approach entails creating a dendrogram. Following its construction, we then proceed to draw a horizontal line that intersects the longest vertical line without a crossing horizontal line in the dendrogram. Upon completion of these steps, we proceed to assess the dendrogram that has been formulated.</p>
                <img class="plot-image" src="../images/subpage3/w.png"/>
                <p>The dendrogram depicted above, which is based on Euclidean distances, suggests that the optimal number of clusters is two. This conclusion is reached by examining the tallest vertical line, or the one with the largest distance, which is the first line or branch. Following this, we'll employ this cluster number in the Agglomerative Clustering algorithm. Subsequently, we will visualize the resultant clusters and carry out an evaluation of their effectiveness. </p>
                <img class="plot-image" src="../images/subpage3/Hiararchical plot.png"/>
                <p>The final phase of our analysis involves evaluating the quality of the clusters generated by the Hierarchical Clustering algorithm. This evaluation will be accomplished using several metrics including the Silhouette Score, Davies-Bouldin Index, and the Calinski-Harabasz Index. These indices will provide a robust assessment of the quality of our clustering solution.</p>
                <img class="plot-evaluate" src="../images/subpage3/Hia Eva.png" />
                <br>
                <br>

                <p><strong style="font-size:24px;">Models Evaluation</strong></p>
                <p>In this section, the quality of the clustering outcomes produced by the algorithm will be evaluated. This table compares the clustering results of each clustering algorithm (Davies-Bouldin index,silhouette score and Calinski-Harabasz Index).</p>
                <img class="plot-image" src="../images/subpage3/Model Comparison.png"/>
                <ul>
                    <li><strong>K-Means Clustering:</strong> K-Means has the lowest Davies-Bouldin Index (0.782), suggesting that the clusters are well-separated. Its silhouette score is the highest (0.461), indicating that the samples are well-clustered. Furthermore, the high Calinski-Harabasz Index (3321.168) suggests that the clusters are dense and well-separated.</li>
                    <li><strong>Hierarchical Clustering:</strong> Hierarchical Clustering has a slightly higher Davies-Bouldin Index (0.884) than K-Means, suggesting less well-separated clusters. Its silhouette score (0.441) is slightly lower, indicating a lower quality of clustering. However, the Calinski-Harabasz Index (2203.244) is still relatively high, indicating that the clusters are dense and well-separated, although not as much as in K-Means.</li>
                    <li><strong>DBSCAN:</strong> DBSCAN has the highest Davies-Bouldin Index (1.56), suggesting poorly separated clusters. Its silhouette score is negative (-0.288), indicating that samples might have been assigned to the wrong clusters. The Calinski-Harabasz Index (224.921) is much lower than the other two models, suggesting that the clusters are not as dense or well-separated.</li>
                </ul>
                <br>
                <p><strong style="font-size:18px;">Conclusion:</strong> According to the evaluation metrics, K-Means Clustering seems to perform best for this specific dataset, followed by Hierarchical Clustering. DBSCAN has the poorest performance according to these metrics. The next step is to conduct profiling to determine the characteristics of each cluster.</p>
                <br>

            <h3>Cluster Profilling</h3>
                <p>This section will identify the characteristics of the clusters created by K-Means through cluster profiling.</p>
                <img class="plot-image" src="../images/subpage3/Profilling.png"/>
                <p>Based on the table above, it can be concluded that each cluster has the following characteristics:</p>
                <ul>
                    <li><span style="text-decoration: underline;">Cluster 1 (Low Income, High Family, Less Engaged Buyers):</span> These customers have the lowest income levels among all clusters. They usually have a high number of children at home (both young kids and teenagers), leading to a large family size. Their online activity is relatively high with frequent website visits, but they make fewer purchases across all channels. Their product preference leans towards wine, with low spending in other product categories. They tend to accept marketing campaign 3 slightly more often than others, but overall campaign acceptance is low.</li>
                  
                    <li><span style="text-decoration: underline;">Cluster 2 (High Income, Single, Variety Buyers):</span> These customers earn the highest income. Most of them do not have children and live alone or with a partner, leading to small family size. They are active buyers across all channels, especially web and catalog, and spend more on all product categories than any other cluster. They are particularly interested in wines and meats. They have the highest acceptance rates for most marketing campaigns, particularly campaign 4 and campaign 5.</li>
                  
                    <li><span style="text-decoration: underline;">Cluster 3 (Mid-High Income, Parent, Balanced Buyers):</span> These customers have above-average income levels and typically have one child at home. They are the most active catalog purchasers and prefer to buy a balanced mix of products, with particular interest in wines and meats. They have above-average campaign acceptance rates, especially for campaign 4.</li>
                  
                    <li><span style="text-decoration: underline;">Cluster 4 (Low-Mid Income, Young, Less Engaged Buyers):</span> These customers have below-average income levels and are generally younger than customers in other clusters. They have a high number of teenagers at home and an average-sized family. Despite frequent website visits, they make fewer purchases, mainly in-store. Their spending is low across all product categories. They have the lowest acceptance rates for marketing campaigns, particularly campaigns 4 and 5.</li>
                  </ul>
                  <br>
                
            <h3>Marketing Strategy Suggestions</h3>
                  <p>Based on the profiling above, the following are suggestions for targeted marketing strategies for each cluster:</p>
                  <br>
                  <p>
                    <strong style="color: #0ef;">Cluster 1 (Low Income, High Family, Less Engaged Buyers):</strong><br>
                    The focus for marketing to this cluster could be on providing value for money and bulk-buying deals. Given their larger family size and lower income, offers could include family-sized bundles or deals on essentials. It could also be beneficial to provide an option for affordable installment plans. As these customers seem to respond more favorably to Campaign 3, this suggests they could be more receptive to direct marketing strategies.
                </p>
                
                <p>
                    <strong style="color: #0ef;">Cluster 2 (High Income, Single, Variety Buyers):</strong><br>
                    With high income and no children, these customers likely have more disposable income and may appreciate higher-end products. The marketing plan could target them with exclusive offers or premium product lines. Given their high activity online and the good response to Campaigns 4 and 5, online marketing and personalized email campaigns could be effective.
                </p>
                
                <p>
                    <strong style="color: #0ef;">Cluster 3 (Mid-High Income, Parent, Balanced Buyers):</strong><br>
                    Customers in this cluster might be interested in diverse product offerings and could appreciate deals on a variety of products. As they show an inclination towards catalog purchases, sending them targeted and personalized catalogs might be effective. Introducing reward or loyalty programs can be a good strategy to motivate them to spend more across different product categories.
                </p>
                
                <p>
                    <strong style="color: #0ef;">Cluster 4 (Low-Mid Income, Young, Less Engaged Buyers):</strong><br>
                    These younger, less engaged buyers might be interested in products that are trendy and affordable. A social media marketing campaign could be a great way to engage this cluster. Also, introducing a referral program could encourage them to bring in friends or family. As they have low interaction with campaigns, especially Campaigns 4 and 5, improving these campaigns' relevance and attractiveness to this demographic might increase engagement.
                </p>
                
                    

                    
                  
                
        
        
        
        </section>
    

    <footer class="footer">
        <div class="footer-text">
            <p>Copyright &copy; 2023 by Viet Cuong Truong | All Rights Reserved.</p>
        </div>

        <div class="footer-iconTop">
            <a href="#projects"><i class='bx bx-up-arrow-alt'></i></a>
        </div>
    </footer>

    <script src="https://unpkg.com/scrollreveal"></script>
    <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>
    <script src="../js/script.js"></script>
</body>

</html>